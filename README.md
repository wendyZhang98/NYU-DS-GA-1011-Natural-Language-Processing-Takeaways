# üß† NYU DS-GA-1011 ‚Äî Natural Language Processing Takeaways

This repository contains **lecture summaries and notes** from *NYU DS-GA-1011: Natural Language Processing*, focusing on the theoretical foundations and modern techniques in NLP.

All notes in the **Lecture** folder were **written by Professor [Kyunghyun Cho](https://cds.nyu.edu/team/kyunghyun-cho-2/)**, and this repository serves as a structured collection of those materials for study and reference.

---

## üìò Overview

This collection provides a concise review of NLP fundamentals, connecting theory and practice through organized lecture content.  
It‚Äôs ideal for students revising the course, researchers refreshing NLP concepts, or practitioners bridging academic NLP with real-world applications.

---

## üß© Key Topics Covered

- **Text Representations:** bag-of-words, TF-IDF, and word embeddings (Word2Vec, GloVe, FastText)  
- **Sequence Models:** RNNs, LSTMs, GRUs, and their limitations  
- **Attention Mechanisms:** self-attention, multi-head attention, and the Transformer architecture  
- **Language Models:** autoregressive and masked language modeling (e.g., GPT, BERT)  
- **NLP Applications:** text classification, sequence labeling, translation, summarization, and question answering  
- **Evaluation Metrics:** perplexity, BLEU, ROUGE, and accuracy  
- **Implementation Notes:** tokenization, batching, fine-tuning strategies, and pretraining tips  

---

## üß† Learning Takeaways

- Understanding the **evolution from classical NLP** (statistical models) to **deep learning-based NLP**  
- Developing intuition for **contextual word representations** and **transfer learning in NLP**  
- Gaining familiarity with **transformer-based architectures** and their scalability  
- Learning best practices for **model evaluation, fine-tuning, and deployment**  

---

## üß≠ How to Use This Repository

1. Navigate to the [Lecture folder](./Lecture).  
2. Select a specific lecture topic (e.g., ‚ÄúTransformer‚Äù, ‚ÄúLanguage Modeling‚Äù).  
3. Review the lecture notes written by **Professor Cho**.  
4. Optionally, apply the concepts by implementing related tasks using PyTorch or Hugging Face.

---

## üìö References & Resources

- [The Illustrated Transformer (Jay Alammar)](https://jalammar.github.io/illustrated-transformer/)  
- [Speech and Language Processing (Jurafsky & Martin)](https://web.stanford.edu/~jurafsky/slp3/)  
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index)  

---

## ‚úçÔ∏è Author

**Wenxin Zhang**  
*NYU DS-GA-1011: Natural Language Processing ‚Äî Lecture Notes by Professor Kyunghyun Cho*
